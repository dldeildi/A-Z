{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvowx9J7D7kC",
        "outputId": "898aaf3a-ab00-410a-d9ff-6e382011e360"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sv738JrND987"
      },
      "outputs": [],
      "source": [
        "# !unzip -qq /content/drive/MyDrive/alphabet.zip -d /content/drive/MyDrive/datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GE2LtDexEES0"
      },
      "outputs": [],
      "source": [
        "# !pip install -q tensorflow-gpu==2.0.0-rc1\n",
        "# !pip install -q tensorflow-gpu==2.8.0-rc1\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "t1DtaKx5SD_g"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import os\n",
        "\n",
        "data_dir = \"/content/drive/MyDrive/datasets/\"\n",
        "# data_dir = pathlib.Path(data_dir)\n",
        "\n",
        "# train_dir = os.path.join(data_dir, 'train')\n",
        "# test_dir = os.path.join(data_dir, 'test')\n",
        "\n",
        "train_dir = \"/content/drive/MyDrive/datasets/train/\"\n",
        "test_dir = \"/content/drive/MyDrive/datasets/test/\"\n",
        "val_dir = \"/content/drive/MyDrive/datasets/validation\"\n",
        "\n",
        "# image_count = len(list(data_dir.glob('A/*.png')))\n",
        "# print(image_count)\n",
        "# image_count = len(list(data_dir.glob('lower_a/*.png')))\n",
        "# print(image_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Nr8N0JCILvtg"
      },
      "outputs": [],
      "source": [
        "# 이미지 데이터 전처리\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Image augmentation\n",
        "# train셋에만 적용\n",
        "# train_datagen = ImageDataGenerator(rescale = 1./255, # 모든 이미지 원소값들을 255로 나누기\n",
        "#                                    rotation_range=25, # 0~25도 사이에서 임의의 각도로 원본이미지를 회전\n",
        "#                                    width_shift_range=0.05, # 0.05범위 내에서 임의의 값만큼 임의의 방향으로 좌우 이동\n",
        "#                                    height_shift_range=0.05, # 0.05범위 내에서 임의의 값만큼 임의의 방향으로 상하 이동\n",
        "#                                    zoom_range=0.2, # (1-0.2)~(1+0.2) => 0.8~1.2 사이에서 임의의 수치만큼 확대/축소\n",
        "#                                    horizontal_flip=True, # 좌우로 뒤집기                                   \n",
        "#                                    vertical_flip=True,\n",
        "#                                    fill_mode='nearest'\n",
        "#                                   ) \n",
        "# validation 및 test 이미지는 augmentation을 적용하지 않는다;\n",
        "# 모델 성능을 평가할 때에는 이미지 원본을 사용 (rescale만 진행)\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "olD5BNgeSV5l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "605b9d1e-2aba-43fe-c68d-92b9b9bab79e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 78052 images belonging to 52 classes.\n",
            "Found 19552 images belonging to 52 classes.\n",
            "Found 47134 images belonging to 52 classes.\n"
          ]
        }
      ],
      "source": [
        "# flow_from_directory() 메서드를 이용해서 훈련과 테스트에 사용될 이미지 데이터를 만들기\n",
        "# 변환된 이미지 데이터 생성\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "                                                    # data_dir,\n",
        "                                                    # \"/content/drive/MyDrive/dataset/\",\n",
        "                                                    train_dir, \n",
        "                                                    batch_size=16, # 한번에 변환된 이미지 16개씩 만들어라 라는 것\n",
        "                                                    color_mode='grayscale', # 흑백 이미지 처리\n",
        "                                                    # class_mode='binary', \n",
        "                                                    class_mode='categorical',\n",
        "                                                    target_size=(150,150)) # target_size에 맞춰서 이미지의 크기가 조절된다\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "                                                              # data_dir,\n",
        "                                                              val_dir,\n",
        "                                                              # \"/content/drive/MyDrive/dataset/\",\n",
        "                                                              batch_size=4, \n",
        "                                                              color_mode='grayscale',\n",
        "                                                              # class_mode='binary', \n",
        "                                                              class_mode='categorical',\n",
        "                                                              target_size=(150,150))\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(test_dir,\n",
        "                                                  batch_size=4,\n",
        "                                                  color_mode='grayscale',\n",
        "                                                  # class_mode='binary',\n",
        "                                                  class_mode='categorical',\n",
        "                                                  target_size=(150,150))\n",
        "# 참고로, generator 생성시 batch_size x steps_per_epoch (model fit에서) <= 훈련 샘플 수 보다 작거나 같아야 한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "a_NSu0uCT2xw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f8e4158-b721-40e5-bc50-12e2127c1a1d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'A': 0,\n",
              " 'B': 1,\n",
              " 'C': 2,\n",
              " 'D': 3,\n",
              " 'E': 4,\n",
              " 'F': 5,\n",
              " 'G': 6,\n",
              " 'H': 7,\n",
              " 'I': 8,\n",
              " 'J': 9,\n",
              " 'K': 10,\n",
              " 'L': 11,\n",
              " 'M': 12,\n",
              " 'N': 13,\n",
              " 'O': 14,\n",
              " 'P': 15,\n",
              " 'Q': 16,\n",
              " 'R': 17,\n",
              " 'S': 18,\n",
              " 'T': 19,\n",
              " 'U': 20,\n",
              " 'V': 21,\n",
              " 'W': 22,\n",
              " 'X': 23,\n",
              " 'Y': 24,\n",
              " 'Z': 25,\n",
              " 'small_a': 26,\n",
              " 'small_b': 27,\n",
              " 'small_c': 28,\n",
              " 'small_d': 29,\n",
              " 'small_e': 30,\n",
              " 'small_f': 31,\n",
              " 'small_g': 32,\n",
              " 'small_h': 33,\n",
              " 'small_i': 34,\n",
              " 'small_j': 35,\n",
              " 'small_k': 36,\n",
              " 'small_l': 37,\n",
              " 'small_m': 38,\n",
              " 'small_n': 39,\n",
              " 'small_o': 40,\n",
              " 'small_p': 41,\n",
              " 'small_q': 42,\n",
              " 'small_r': 43,\n",
              " 'small_s': 44,\n",
              " 'small_t': 45,\n",
              " 'small_u': 46,\n",
              " 'small_v': 47,\n",
              " 'small_w': 48,\n",
              " 'small_x': 49,\n",
              " 'small_y': 50,\n",
              " 'small_z': 51}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# class 확인\n",
        "train_generator.class_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "b4ui5ErnU0bm"
      },
      "outputs": [],
      "source": [
        "# # 합성곱 신경망 모델 구성하기\n",
        "# import tensorflow as tf\n",
        "\n",
        "# model = tf.keras.models.Sequential([\n",
        "#     tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 1)),\n",
        "#     tf.keras.layers.MaxPooling2D(2,2),\n",
        "#     tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "#     tf.keras.layers.MaxPooling2D(2,2),\n",
        "#     tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "#     tf.keras.layers.MaxPooling2D(2,2),\n",
        "#     tf.keras.layers.Flatten(),\n",
        "#     tf.keras.layers.Dense(512, activation='relu'),\n",
        "#     # tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "#     tf.keras.layers.Dense(52, activation='softmax')\n",
        "# ])\n",
        "# model.summary() # 신경망의 구조 확인"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 합성곱 신경망 모델 구성하기\n",
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 1)),\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    # tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    tf.keras.layers.Dense(52, activation='softmax')\n",
        "])\n",
        "model.summary() # 신경망의 구조 확인"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUH2OG9UVsx_",
        "outputId": "d16c67fe-13a7-4291-9052-fb2ead8ca6ee"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 148, 148, 16)      160       \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 146, 146, 16)      2320      \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 73, 73, 16)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 71, 71, 32)        4640      \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 35, 35, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 33, 33, 32)        9248      \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 16, 16, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 14, 14, 64)        18496     \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 12, 12, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 6, 6, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 4, 4, 128)         73856     \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, 2, 2, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 52)                26676     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 434,980\n",
            "Trainable params: 434,980\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "yadtiWbEV4YA"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "# compile() 메서드를 이용해서 손실 함수 (loss function)와 옵티마이저 (optimizer)를 지정\n",
        "model.compile(optimizer=RMSprop(learning_rate=0.001), # 옵티마이저로는 RMSprop 사용\n",
        "              loss='binary_crossentropy', # 손실 함수로 ‘binary_crossentropy’ 사용\n",
        "              metrics= ['accuracy'])\n",
        "# RMSprop (Root Mean Square Propagation) Algorithm: 훈련 과정 중에 학습률을 적절하게 변화시킨다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4qJWP-BrV9yK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bbc67b1-c3e6-4dd5-f756-6666ba5b4883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  import sys\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "52/52 - 235s - loss: 0.1408 - accuracy: 0.0216 - val_loss: 0.1104 - val_accuracy: 0.0000e+00 - 235s/epoch - 5s/step\n",
            "Epoch 2/30\n",
            "52/52 - 217s - loss: 0.1072 - accuracy: 0.0180 - val_loss: 0.1030 - val_accuracy: 0.0000e+00 - 217s/epoch - 4s/step\n",
            "Epoch 3/30\n",
            "52/52 - 213s - loss: 0.1024 - accuracy: 0.0337 - val_loss: 0.0928 - val_accuracy: 0.1750 - 213s/epoch - 4s/step\n",
            "Epoch 4/30\n",
            "52/52 - 211s - loss: 0.0918 - accuracy: 0.1286 - val_loss: 0.0835 - val_accuracy: 0.2750 - 211s/epoch - 4s/step\n",
            "Epoch 5/30\n",
            "52/52 - 203s - loss: 0.0771 - accuracy: 0.2464 - val_loss: 0.0662 - val_accuracy: 0.3000 - 203s/epoch - 4s/step\n",
            "Epoch 6/30\n",
            "52/52 - 211s - loss: 0.0639 - accuracy: 0.3630 - val_loss: 0.0594 - val_accuracy: 0.4000 - 211s/epoch - 4s/step\n",
            "Epoch 7/30\n",
            "52/52 - 212s - loss: 0.0519 - accuracy: 0.4579 - val_loss: 0.0548 - val_accuracy: 0.4750 - 212s/epoch - 4s/step\n",
            "Epoch 8/30\n",
            "52/52 - 202s - loss: 0.0491 - accuracy: 0.4622 - val_loss: 0.0455 - val_accuracy: 0.6000 - 202s/epoch - 4s/step\n",
            "Epoch 9/30\n",
            "52/52 - 197s - loss: 0.0446 - accuracy: 0.5048 - val_loss: 0.0326 - val_accuracy: 0.6750 - 197s/epoch - 4s/step\n",
            "Epoch 10/30\n",
            "52/52 - 191s - loss: 0.0426 - accuracy: 0.5541 - val_loss: 0.0408 - val_accuracy: 0.5750 - 191s/epoch - 4s/step\n",
            "Epoch 11/30\n",
            "52/52 - 183s - loss: 0.0411 - accuracy: 0.5709 - val_loss: 0.0468 - val_accuracy: 0.4750 - 183s/epoch - 4s/step\n",
            "Epoch 12/30\n",
            "52/52 - 182s - loss: 0.0373 - accuracy: 0.5938 - val_loss: 0.0285 - val_accuracy: 0.6750 - 182s/epoch - 4s/step\n",
            "Epoch 13/30\n",
            "52/52 - 185s - loss: 0.0387 - accuracy: 0.5769 - val_loss: 0.0357 - val_accuracy: 0.6250 - 185s/epoch - 4s/step\n",
            "Epoch 14/30\n",
            "52/52 - 179s - loss: 0.0357 - accuracy: 0.5938 - val_loss: 0.0334 - val_accuracy: 0.6000 - 179s/epoch - 3s/step\n",
            "Epoch 15/30\n",
            "52/52 - 178s - loss: 0.0360 - accuracy: 0.5925 - val_loss: 0.0365 - val_accuracy: 0.6500 - 178s/epoch - 3s/step\n",
            "Epoch 16/30\n",
            "52/52 - 178s - loss: 0.0332 - accuracy: 0.6526 - val_loss: 0.0259 - val_accuracy: 0.7250 - 178s/epoch - 3s/step\n",
            "Epoch 17/30\n",
            "52/52 - 172s - loss: 0.0329 - accuracy: 0.6402 - val_loss: 0.0324 - val_accuracy: 0.6250 - 172s/epoch - 3s/step\n",
            "Epoch 18/30\n",
            "52/52 - 176s - loss: 0.0333 - accuracy: 0.6106 - val_loss: 0.0289 - val_accuracy: 0.6500 - 176s/epoch - 3s/step\n",
            "Epoch 19/30\n",
            "52/52 - 184s - loss: 0.0333 - accuracy: 0.6166 - val_loss: 0.0328 - val_accuracy: 0.5000 - 184s/epoch - 4s/step\n",
            "Epoch 20/30\n",
            "52/52 - 169s - loss: 0.0324 - accuracy: 0.6406 - val_loss: 0.0362 - val_accuracy: 0.4500 - 169s/epoch - 3s/step\n",
            "Epoch 21/30\n",
            "52/52 - 176s - loss: 0.0310 - accuracy: 0.6683 - val_loss: 0.0271 - val_accuracy: 0.6500 - 176s/epoch - 3s/step\n",
            "Epoch 22/30\n",
            "52/52 - 173s - loss: 0.0291 - accuracy: 0.6562 - val_loss: 0.0305 - val_accuracy: 0.6500 - 173s/epoch - 3s/step\n",
            "Epoch 23/30\n",
            "52/52 - 167s - loss: 0.0319 - accuracy: 0.6354 - val_loss: 0.0343 - val_accuracy: 0.5750 - 167s/epoch - 3s/step\n",
            "Epoch 24/30\n",
            "52/52 - 167s - loss: 0.0304 - accuracy: 0.6394 - val_loss: 0.0266 - val_accuracy: 0.7500 - 167s/epoch - 3s/step\n",
            "Epoch 25/30\n",
            "52/52 - 166s - loss: 0.0302 - accuracy: 0.6406 - val_loss: 0.0236 - val_accuracy: 0.7500 - 166s/epoch - 3s/step\n",
            "Epoch 26/30\n",
            "52/52 - 173s - loss: 0.0290 - accuracy: 0.6695 - val_loss: 0.0293 - val_accuracy: 0.6500 - 173s/epoch - 3s/step\n",
            "Epoch 27/30\n",
            "52/52 - 151s - loss: 0.0292 - accuracy: 0.6671 - val_loss: 0.0329 - val_accuracy: 0.6750 - 151s/epoch - 3s/step\n",
            "Epoch 28/30\n",
            "52/52 - 153s - loss: 0.0298 - accuracy: 0.6611 - val_loss: 0.0354 - val_accuracy: 0.6500 - 153s/epoch - 3s/step\n",
            "Epoch 29/30\n",
            "52/52 - 154s - loss: 0.0300 - accuracy: 0.6683 - val_loss: 0.0236 - val_accuracy: 0.6250 - 154s/epoch - 3s/step\n",
            "Epoch 30/30\n",
            "52/52 - 157s - loss: 0.0291 - accuracy: 0.6526 - val_loss: 0.0399 - val_accuracy: 0.6000 - 157s/epoch - 3s/step\n"
          ]
        }
      ],
      "source": [
        "# 모델 훈련\n",
        "history = model.fit_generator(train_generator, # train_generator안에 X값, y값 다 있으니 generator만 주면 된다\n",
        "                              validation_data=validation_generator, # validatino_generator안에도 검증용 X,y데이터들이 다 있으니 generator로 주면 됨\n",
        "                              steps_per_epoch=52, # 한 번의 에포크(epoch)에서 훈련에 사용할 배치(batch)의 개수 지정; generator를 4번 부르겠다\n",
        "                              epochs=30, # 데이터셋을 한 번 훈련하는 과정; epoch은 100 이상은 줘야한다\n",
        "                              validation_steps=10, # 한 번의 에포크가 끝날 때, 검증에 사용되는 배치(batch)의 개수를 지정; validation_generator를 4번 불러서 나온 이미지들로 작업을 해라\n",
        "                              verbose=2)\n",
        "# 참고: validation_steps는 보통 내가 원하는 이미지 수에 flow할 때 지정한 batchsize로 나눈 값을 validation_steps로 지정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CXqugpj_lgz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "251f2b8e-008f-4659-de65-05e48a72a2e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  33/4879 [..............................] - ETA: 3:52:39 - loss: 0.0294 - accuracy: 0.6288"
          ]
        }
      ],
      "source": [
        "# 모델 성능 평가\n",
        "model.evaluate(train_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnfmhRbV_mK9"
      },
      "outputs": [],
      "source": [
        "model.evaluate(validation_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hyoWBzT8_F8"
      },
      "outputs": [],
      "source": [
        "# 이제 테스트 이미지 분류\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "\n",
        "# 테스트용 A 이미지 경로 설정\n",
        "test_dir = '/content/drive/MyDrive/datasets/test/'\n",
        "test_A_dir = os.path.join(test_dir, 'A/')\n",
        "test_A_filenames = os.listdir(test_A_dir)\n",
        "test_A_filenames\n",
        "\n",
        "# 테스트용 a 이미지 경로 설정\n",
        "test_dir = '/content/drive/MyDrive/datasets/test/'\n",
        "test_a_dir = os.path.join(test_dir, 'small_a/')\n",
        "test_a_filenames = os.listdir(test_a_dir)\n",
        "test_a_filenames\n",
        "\n",
        "# A,a를 key로, 이미지 파일 이름들을 value로 dictionary 생성\n",
        "dic_Aa_filenames = {}\n",
        "dic_Aa_filenames['A'] = test_A_filenames\n",
        "dic_Aa_filenames['a'] = test_a_filenames\n",
        "\n",
        "# A/a 분류 테스트\n",
        "for Aa, filenames in dic_Aa_filenames.items():\n",
        "    fig = plt.figure(figsize=(16,10))\n",
        "    rows, cols = 1, 6\n",
        "    for i, fn in enumerate(filenames):\n",
        "        path = test_dir + Aa + '/' + fn\n",
        "        test_img = image.load_img(path, color_mode='grayscale', target_size=(150, 150), interpolation='bilinear')        \n",
        "        x = image.img_to_array(test_img)\n",
        "        x = np.expand_dims(x, axis=0)\n",
        "        images = np.vstack([x])\n",
        "\n",
        "        classes = model.predict(images, batch_size=10)\n",
        "        \n",
        "        fig.add_subplot(rows, cols, i+1)\n",
        "        if classes[0]==0:\n",
        "            plt.title(fn + \" is A\")\n",
        "            plt.axis('off')\n",
        "            plt.imshow(test_img, cmap='gray')\n",
        "\n",
        "        else:\n",
        "            plt.title(fn + \" is a\")\n",
        "            plt.axis('off')\n",
        "            plt.imshow(test_img, cmap='gray')\n",
        "    plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPQ2iK1S_umL"
      },
      "outputs": [],
      "source": [
        "# 모델 성능 평가\n",
        "model.evaluate(test_generator)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled7.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNBO3esarqXExvnIbZVq5ml"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}